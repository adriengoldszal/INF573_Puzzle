{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yolov5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      6\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../yolov5\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Update to your YOLOv5 path\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myolov5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DetectMultiBackend\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myolov5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneral\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m non_max_suppression\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myolov5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m select_device\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yolov5'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../yolov5')  # Update to your YOLOv5 path\n",
    "\n",
    "from yolov5.models.common import DetectMultiBackend\n",
    "from yolov5.utils.general import non_max_suppression\n",
    "from yolov5.utils.torch_utils import select_device\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "def scale_coords(img1_shape, coords, img0_shape, ratio_pad=None):\n",
    "    \"\"\"\n",
    "    Rescales bounding box coordinates from one image shape to another.\n",
    "\n",
    "    Args:\n",
    "        img1_shape (tuple): Shape of the resized image used for inference (height, width).\n",
    "        coords (torch.Tensor): Bounding box coordinates as (x1, y1, x2, y2).\n",
    "        img0_shape (tuple): Original image shape (height, width).\n",
    "        ratio_pad (tuple, optional): Pre-computed gain and padding values.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Scaled bounding box coordinates.\n",
    "    \"\"\"\n",
    "    if ratio_pad is None:  # Calculate from shapes\n",
    "        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain = resized / original\n",
    "        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # width, height\n",
    "    else:\n",
    "        gain, pad = ratio_pad\n",
    "\n",
    "    coords[:, [0, 2]] -= pad[0]  # x padding\n",
    "    coords[:, [1, 3]] -= pad[1]  # y padding\n",
    "    coords[:, :4] /= gain\n",
    "    coords[:, :4].clamp_(min=0, max=max(img0_shape))  # Ensure coordinates stay within image bounds\n",
    "    return coords\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def resize_and_crop(image, target_size, alpha=1.0):\n",
    "    orig_h, orig_w = image.shape[:2]\n",
    "    target_w, target_h = target_size\n",
    "\n",
    "    # Scaling\n",
    "    scale = max(target_w / (orig_w * alpha), target_h / (orig_h * alpha))\n",
    "    new_w, new_h = int(orig_w * scale), int(orig_h * scale)\n",
    "\n",
    "    # Resizing\n",
    "    resized_image = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Center cropping\n",
    "    x1 = (new_w - target_w) // 2\n",
    "    y1 = (new_h - target_h) // 2\n",
    "    cropped_image = resized_image[y1:y1 + target_h, x1:x1 + target_w]\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "# Set up device\n",
    "device = select_device('')  # Automatically selects CUDA if available, otherwise CPU\n",
    "\n",
    "# Load the trained model\n",
    "model = DetectMultiBackend(\n",
    "    '/Users/martindrieux/Documents/GitHub/INF573_Puzzle-1/yolov5/runs/train/exp2/weights/best.pt',\n",
    "    device=device,\n",
    "    dnn=False\n",
    ")\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = '../nos_puzzles/yakari_pieces1.png'\n",
    "yakari_pieces = cv2.imread(image_path)\n",
    "yakari_pieces = cv2.cvtColor(yakari_pieces, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "target_size = (640, 640)\n",
    "img_resized = resize_and_crop(yakari_pieces, target_size, alpha=0.9)\n",
    "\n",
    "# Normalize and convert to PyTorch tensor\n",
    "img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).unsqueeze(0).float().div(255.0).to(device)\n",
    "\n",
    "# Run inference\n",
    "predictions = model(img_tensor)\n",
    "\n",
    "# Apply Non-Max Suppression (NMS)\n",
    "results = non_max_suppression(predictions, conf_thres=0.05, iou_thres=0.45)\n",
    "\n",
    "# Annotate and display results\n",
    "annotated_image = img_resized.copy()\n",
    "for det in results:\n",
    "    if det is not None and len(det):\n",
    "        # Scale coordinates back to the original resized image\n",
    "        det[:, :4] = scale_coords(img_tensor.shape[2:], det[:, :4], annotated_image.shape).round()\n",
    "\n",
    "        for *xyxy, conf, cls in det:\n",
    "            # Draw bounding boxes and labels\n",
    "            label = f'{model.names[int(cls)]} {conf:.2f}'\n",
    "            x_min, y_min, x_max, y_max = map(int, xyxy)\n",
    "            cv2.rectangle(annotated_image, (x_min, y_min), (x_max, y_max), color=(0, 255, 0), thickness=2)\n",
    "            cv2.putText(\n",
    "                annotated_image, label, (x_min, y_min - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, color=(0, 255, 0), thickness=2\n",
    "            )\n",
    "\n",
    "# Display the annotated image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(annotated_image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x3002d7a70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_model(img_resized, conf_t, iou_t):\n",
    "    img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).unsqueeze(0).float().div(255.0).to(device)\n",
    "    predictions = model(img_tensor)\n",
    "    results = non_max_suppression(predictions, conf_thres=conf_t, iou_thres=iou_t)\n",
    "    return predictions, results\n",
    "\n",
    "def draw_boxes(img, results):\n",
    "    annotated_image = img.copy()\n",
    "    for det in results:\n",
    "        if det is not None and len(det):\n",
    "            det[:, :4] = scale_coords(img_tensor.shape[2:], det[:, :4], annotated_image.shape).round()\n",
    "            for *xyxy, conf, cls in det:\n",
    "                if len(xyxy) != 4:\n",
    "                    print(\"Invalid detection skipped:\", xyxy)  # Debugging output\n",
    "                    continue  # Skip invalid detections\n",
    "                \n",
    "                label = f'{model.names[int(cls)]} {conf:.2f}'\n",
    "                x_min, y_min, x_max, y_max = map(int, xyxy)\n",
    "                cv2.rectangle(annotated_image, (x_min, y_min), (x_max, y_max), color=(0, 255, 0), thickness=2)\n",
    "                cv2.putText(\n",
    "                    annotated_image, label, (x_min, y_min - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, color=(0, 255, 0), thickness=2\n",
    "                )\n",
    "    return annotated_image\n",
    "\n",
    "\n",
    "pred, res = apply_model(img_resized, 0.05, 0.45)\n",
    "draw_boxes(img_resized, res)\n",
    "plt.imshow(draw_boxes(img_resized, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_aggregate(image, overlap=0.2, conf_thres=0.25, iou_thres=0.45, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Detect bounding boxes from overlapping sub-images and aggregate them into the original image.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image in RGB format.\n",
    "        overlap (float): Fractional overlap between sub-images.\n",
    "        conf_thres (float): Confidence threshold for detection.\n",
    "        iou_thres (float): IoU threshold for Non-Max Suppression.\n",
    "        alpha (float): Scaling factor for resizing sub-images.\n",
    "                      For alpha=1, sub-image is a square of the smallest dimension of the image.\n",
    "\n",
    "    Returns:\n",
    "        list: Aggregated bounding boxes in the format [x_min, y_min, x_max, y_max, confidence].\n",
    "        numpy.ndarray: Annotated image with bounding boxes drawn.\n",
    "    \"\"\"\n",
    "    orig_h, orig_w = image.shape[:2]\n",
    "    smallest_dim = min(orig_h, orig_w)  # Use the smallest dimension as the base size\n",
    "    sub_size = int(smallest_dim * alpha)  # Adjust sub-image size based on alpha\n",
    "\n",
    "    # Ensure sub-size is valid\n",
    "    min_sub_size = 32  # Minimum acceptable size\n",
    "    sub_size = max(sub_size, min_sub_size)\n",
    "\n",
    "    step = int(sub_size * (1 - overlap))  # Calculate the step size based on overlap\n",
    "    step = max(step, 1)  # Ensure step size is at least 1 pixel\n",
    "\n",
    "    all_boxes = []\n",
    "\n",
    "    # Loop through sub-images\n",
    "    for y in range(0, orig_h, step):\n",
    "        for x in range(0, orig_w, step):\n",
    "            # Adjust sub-image coordinates to stay within the image boundaries\n",
    "            x_end = min(x + sub_size, orig_w)\n",
    "            y_end = min(y + sub_size, orig_h)\n",
    "\n",
    "            # Adjust overlap for sub-images near the edges\n",
    "            if x + sub_size > orig_w:\n",
    "                overlap_x = (x + sub_size - orig_w) / sub_size\n",
    "                x = int(x - sub_size * overlap_x)  # Adjust x to increase overlap\n",
    "            if y + sub_size > orig_h:\n",
    "                overlap_y = (y + sub_size - orig_h) / sub_size\n",
    "                y = int(y - sub_size * overlap_y)  # Adjust y to increase overlap\n",
    "\n",
    "            # Define the sub-image\n",
    "            sub_image = image[y:y_end, x:x_end]\n",
    "\n",
    "            # Resize sub-image to match the required detection size\n",
    "            sub_image_resized = cv2.resize(sub_image, (sub_size, sub_size))\n",
    "\n",
    "            # Detect objects in the sub-image\n",
    "            _, results = apply_model(sub_image_resized, conf_thres, iou_thres)\n",
    "\n",
    "            # Adjust bounding boxes to original image coordinates\n",
    "            for det in results:\n",
    "                if det is not None and len(det):\n",
    "                    det[:, :4] = scale_coords((sub_size, sub_size), det[:, :4], sub_image_resized.shape).round()\n",
    "                    for *xyxy, conf, cls in det:\n",
    "                        x_min, y_min, x_max, y_max = map(int, xyxy)\n",
    "                        x_min += x\n",
    "                        x_max += x\n",
    "                        y_min += y\n",
    "                        y_max += y\n",
    "                        all_boxes.append([x_min, y_min, x_max, y_max, conf.item()])\n",
    "\n",
    "    # Aggregate bounding boxes using Non-Max Suppression\n",
    "    if len(all_boxes) > 0:\n",
    "        boxes = torch.tensor(all_boxes)[:, :4]\n",
    "        scores = torch.tensor(all_boxes)[:, 4]\n",
    "        nms_indices = torch.ops.torchvision.nms(boxes, scores, iou_thres)\n",
    "        all_boxes = np.array(all_boxes)[nms_indices.numpy()]\n",
    "\n",
    "    # Annotate the original image\n",
    "    annotated_image = image.copy()\n",
    "    for x_min, y_min, x_max, y_max, conf in all_boxes:\n",
    "        x_min, y_min, x_max, y_max = map(int, [x_min, y_min, x_max, y_max])\n",
    "        label = f'Conf: {conf:.2f}'\n",
    "        cv2.rectangle(annotated_image, (x_min, y_min), (x_max, y_max), color=(0, 255, 0), thickness=2)\n",
    "        cv2.putText(\n",
    "            annotated_image, label, (x_min, y_min - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, color=(0, 255, 0), thickness=2\n",
    "        )\n",
    "\n",
    "    return all_boxes, annotated_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "original_image = yakari_pieces\n",
    "original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Test with small alpha values\n",
    "\n",
    "\n",
    "bounding_boxes, annotated_image = detect_and_aggregate(\n",
    "    original_image, overlap=0.2, conf_thres=0.05, iou_thres=0.45, alpha=1\n",
    ")\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(annotated_image)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_rgba\n",
    "import numpy as np\n",
    "\n",
    "def visualize_sub_images(image, overlap=0.2, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Visualize sub-images by overlaying semi-transparent, colored boxes on the original image,\n",
    "    accounting for a zooming parameter (alpha).\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image in RGB format.\n",
    "        overlap (float): Fractional overlap between sub-images.\n",
    "        alpha (float): Scaling parameter for sub-image size.\n",
    "                      For alpha=1, sub-image is a square of the smallest dimension of the image.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the visualization.\n",
    "    \"\"\"\n",
    "    orig_h, orig_w = image.shape[:2]\n",
    "    smallest_dim = min(orig_h, orig_w)  # Use the smallest dimension as the base size\n",
    "    sub_size = int(smallest_dim * alpha)  # Adjust sub-image size based on alpha\n",
    "\n",
    "    # Ensure sub-size doesn't exceed the original dimensions\n",
    "    sub_size = min(sub_size, orig_h, orig_w)\n",
    "\n",
    "    step = int(sub_size * (1 - overlap))  # Calculate the step size based on overlap\n",
    "\n",
    "    # Define a set of colors\n",
    "    colors = [\n",
    "        \"red\", \"blue\", \"green\", \"orange\", \"purple\", \"yellow\", \"cyan\", \"magenta\"\n",
    "    ]\n",
    "    color_count = len(colors)\n",
    "\n",
    "    # Plot the original image\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Loop through sub-images and draw boxes\n",
    "    for idx_y, y in enumerate(range(0, orig_h, step)):\n",
    "        for idx_x, x in enumerate(range(0, orig_w, step)):\n",
    "            # Define the sub-image coordinates\n",
    "            x_end = min(x + sub_size, orig_w)\n",
    "            y_end = min(y + sub_size, orig_h)\n",
    "            width = x_end - x\n",
    "            height = y_end - y\n",
    "\n",
    "            # Choose a color cyclically from the list\n",
    "            color = colors[(idx_y * len(range(0, orig_w, step)) + idx_x) % color_count]\n",
    "\n",
    "            # Create a semi-transparent rectangle\n",
    "            rect = patches.Rectangle(\n",
    "                (x, y), width, height,\n",
    "                linewidth=1, edgecolor=color,\n",
    "                facecolor=to_rgba(color, 0.3)  # Semi-transparent fill\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your image\n",
    "original_image = yakari_pieces\n",
    "original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Visualize sub-images with zooming (alpha)\n",
    "visualize_sub_images(original_image, overlap=0.2, alpha=0.5)  # Largest sub-image size\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
